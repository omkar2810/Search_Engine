{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import functools\n",
    "import xml.sax\n",
    "import re\n",
    "from spacy.lang.en import English\n",
    "from Stemmer import Stemmer\n",
    "from collections import defaultdict\n",
    "nlp = English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_index = defaultdict(lambda: defaultdict(lambda : defaultdict(int)))\n",
    "posting_list = defaultdict(lambda:[])\n",
    "tfdict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(string):\n",
    "    tokens = re.split(r'[^A-Za-z0-9]+',string)\n",
    "    return tokens\n",
    "def remove_stop(tokens):\n",
    "    arr = []\n",
    "    for t in tokens:\n",
    "        t = str(t)\n",
    "        if t =='\\n':\n",
    "            continue\n",
    "        if len(t) > 8:\n",
    "            try:\n",
    "                int(t,16)\n",
    "                continue\n",
    "            except:\n",
    "                pass\n",
    "        if len(t) != 0 and nlp.vocab[t].is_stop == False:\n",
    "            arr.append(t.lower())\n",
    "    return arr\n",
    "def stem_words(tokens):\n",
    "    arr = []\n",
    "    ps  = Stemmer('porter')\n",
    "    for token in tokens:\n",
    "        arr.append(ps.stemWord(token))\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_title(data,cur_id):\n",
    "    tf = 0\n",
    "    tokens = tokenize(data)\n",
    "    tokens = remove_stop(tokens)\n",
    "    tokens = stem_words(tokens)\n",
    "    for tok in tokens:\n",
    "        inv_index[tok][cur_id]['t'] += 1\n",
    "        tf += 1\n",
    "    return tf\n",
    "\n",
    "def parse_coll(data,cur_id):\n",
    "    try:   \n",
    "        tf = 0\n",
    "        categories = re.findall(\"\\[\\[Category:(.*?)\\]\\]\", data)\n",
    "        for cat in categories:\n",
    "            tokens = tokenize(cat)\n",
    "            tokens = remove_stop(tokens)\n",
    "            tokens = stem_words(tokens)\n",
    "            for tok in tokens:\n",
    "                inv_index[tok][cur_id]['c'] += 1\n",
    "                tf += 1\n",
    "    except:\n",
    "        tokens = []\n",
    "    return tf   \n",
    "    \n",
    "def parse_refs(data,cur_id):   \n",
    "    try:\n",
    "        tokens = []\n",
    "        tf = 0\n",
    "        link_list = re.findall(\"== *[Rr]eferences *==(.*?\\n)+?\\n\",data)\n",
    "        for link in link_list:\n",
    "            if 'Category' in link or 'reflist' in link or 'Reflist' in link or 'refend' in link:\n",
    "                continue\n",
    "            tokens = tokenize(link)\n",
    "            tokens = remove_stop(tokens)\n",
    "            tokens = stem_words(tokens)\n",
    "            for tok in tokens:\n",
    "                inv_index[tok][cur_id]['r'] += 1\n",
    "                tf += 1\n",
    "    except:\n",
    "        pass\n",
    "    return tf\n",
    "        \n",
    "def parse_links(data,cur_id):\n",
    "    try:\n",
    "        a = data.split(\"==External links==\",1)\n",
    "        tokens = []\n",
    "        tf = 0\n",
    "        link_list = re.findall(\"\\*{{(.*?)}}\",a[1])\n",
    "        for link in link_list:\n",
    "            tokens = tokenize(link)\n",
    "            tokens = remove_stop(tokens)\n",
    "            tokens = stem_words(tokens)\n",
    "            for tok in tokens:\n",
    "                inv_index[tok][cur_id]['l'] += 1\n",
    "                tf += 1\n",
    "    except:\n",
    "        pass\n",
    "    return tf\n",
    "\n",
    "def parse_info(data,cur_id):\n",
    "    try:\n",
    "        tf = 0\n",
    "        info = data.split('{{Infobox')\n",
    "        for toki in info:\n",
    "            link_list = toki.split('\\n')\n",
    "            for link in link_list:\n",
    "                if \"=\" in link:\n",
    "                    tokens = link.split(\"=\")[1]\n",
    "                    tokens = tokenize(tokens)\n",
    "                    tokens = remove_stop(tokens)\n",
    "                    tokens = stem_words(tokens)\n",
    "                    for tok in tokens:\n",
    "                        inv_index[tok][cur_id]['i'] += 1\n",
    "                        tf += 1\n",
    "    except:\n",
    "        pass\n",
    "    return tf\n",
    "\n",
    "def parse_text(data,cur_id):\n",
    "    tokens = []\n",
    "    tf = 0\n",
    "    try:\n",
    "        tokens = tokenize(data)\n",
    "        tokens = remove_stop(tokens)\n",
    "        tokens = stem_words(tokens)\n",
    "    except:\n",
    "        tokens = []\n",
    "    for tok in tokens:\n",
    "        inv_index[tok][cur_id]['b'] += 1\n",
    "        tf += 1\n",
    "    return tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index():\n",
    "    words = sorted(inv_index.keys())\n",
    "    for word in words:\n",
    "        v1 = inv_index[word]\n",
    "        for doc,v2 in v1.items():\n",
    "            val = doc\n",
    "            for typ in sorted(v2): \n",
    "                val += typ+str(v2[typ])\n",
    "            posting_list[word].append(val)\n",
    "\n",
    "def write_to_file(filename):\n",
    "    with open(filename,'w+') as fil:\n",
    "        for key in posting_list.keys():\n",
    "            fil.write(key+\"|\")\n",
    "            for entry in  posting_list[key]:\n",
    "                fil.write(entry+\"|\")\n",
    "            fil.write(\"\\n\")\n",
    "\n",
    "def write_tf(filename):\n",
    "    with open(filename,'w+') as fil:\n",
    "        for key in tfdict.keys():\n",
    "            fil.write(str(key)+'|'+str(tfdict[key][0])+'|'+str(tfdict[key][1])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WikiHandler(xml.sax.ContentHandler):\n",
    "    def __init__(self):\n",
    "        inv_index.clear()\n",
    "        posting_list.clear()\n",
    "        self.data = \"\"\n",
    "        self.title = \"\"\n",
    "        self.cur_id = 0\n",
    "        self.limit = 1000\n",
    "        self.count = 0\n",
    "        self.tf = 0\n",
    "        \n",
    "    def startElement(self, tag, attributes):\n",
    "        self.data = \"\"\n",
    "        \n",
    "    def endElement(self, tag):\n",
    "        if tag == 'page':\n",
    "            if (self.cur_id+1) % self.limit == 0:\n",
    "                create_index()\n",
    "                write_to_file(\"index/\"+str(self.count)+\".txt\")\n",
    "                posting_list.clear()\n",
    "                inv_index.clear()\n",
    "                self.count += 1\n",
    "            tfdict[self.cur_id] = [self.title,str(self.tf)]\n",
    "            self.tf = 0\n",
    "            self.title = \"\"\n",
    "            self.cur_id += 1\n",
    "        \n",
    "        elif tag == 'text':\n",
    "            self.tf += parse_text(self.data,str(self.cur_id))\n",
    "            self.tf += parse_coll(self.data,str(self.cur_id))\n",
    "            self.tf += parse_links(self.data,str(self.cur_id))\n",
    "            self.tf += parse_refs(self.data,str(self.cur_id))\n",
    "            self.tf += parse_info(self.data,str(self.cur_id))\n",
    "            self.data = \"\"\n",
    "\n",
    "        elif tag == 'title':\n",
    "            self.title = self.data\n",
    "            self.tf += parse_title(self.data,str(self.cur_id))\n",
    "            self.data = \"\"\n",
    "\n",
    "    def characters(self, content):\n",
    "        self.data += content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = xml.sax.make_parser()\n",
    "parser.setFeature(xml.sax.handler.feature_namespaces, 0)\n",
    "\n",
    "Handler = WikiHandler()\n",
    "parser.setContentHandler(Handler)\n",
    "parser.parse('enwiki-latest-pages-articles26.xml-p42567204p42663461')\n",
    "create_index()\n",
    "write_to_file(\"index/\"+str(Handler.count)+\".txt\")\n",
    "write_tf(\"index/tf.txt\")\n",
    "merge_files('index/')\n",
    "unmerge_files('index/','0.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comp(a):\n",
    "    return int(a.split('.')[0])\n",
    "\n",
    "def unmerge_files(folder,filename):\n",
    "    thresh = 100000\n",
    "    os.rename(folder+filename,folder+'temp.txt')\n",
    "    cur = 0\n",
    "    file1 = open(folder+'temp.txt','r')\n",
    "    file2 = open(folder+'sindex.txt','w+')\n",
    "    line = file1.readline()\n",
    "    while len(line):\n",
    "        count = 1\n",
    "        file2.write(str(cur)+'.txt|'+line.split('|')[0]+'\\n')\n",
    "        with open(folder+str(cur)+'.txt','w+') as fil:\n",
    "            while count <= thresh and len(line):\n",
    "                fil.write(line)\n",
    "                count += 1\n",
    "                line = file1.readline()\n",
    "        cur += 1\n",
    "    file1.close()\n",
    "    file2.close()\n",
    "    os.remove(folder+'temp.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_files(folder):\n",
    "    while(1):\n",
    "        files = os.listdir(folder)\n",
    "        files.remove('tf.txt')\n",
    "        files = sorted(files,key=comp)\n",
    "        if len(files) == 1:\n",
    "            break\n",
    "        for i in range(1,len(files),2):\n",
    "            merge_func(folder+files[i-1],folder+files[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_func(file1,file2):\n",
    "    \n",
    "    fil1 = open(file1,'r')\n",
    "    fil2 = open(file2,'r')\n",
    "    final = open('index/temp.txt','w+')\n",
    "    line1 = fil1.readline().strip('\\n')\n",
    "    line2 = fil2.readline().strip('\\n')\n",
    "    \n",
    "    while len(line1) and len(line2):\n",
    "        w1 = line1.split(\"|\")[0]\n",
    "        w2 = line2.split(\"|\")[0]\n",
    "        if w1 < w2:\n",
    "            final.write(line1+'\\n')\n",
    "            line1 = fil1.readline().strip('\\n')\n",
    "        elif w2 < w1:\n",
    "            final.write(line2+'\\n')\n",
    "            line2 = fil2.readline().strip('\\n')\n",
    "        else:\n",
    "            temp = \"|\".join(line2.split(\"|\")[1:-1])\n",
    "            final.write(line1+temp+\"|\"+'\\n')\n",
    "            line1 = fil1.readline().strip('\\n')\n",
    "            line2 = fil2.readline().strip('\\n')\n",
    "    \n",
    "    while len(line1):\n",
    "        final.write(line1+'\\n')\n",
    "        line1 = fil1.readline().strip('\\n')\n",
    "    \n",
    "    while len(line2):\n",
    "        final.write(line2+'\\n')\n",
    "        line2  = fil2.readline().strip('\\n')\n",
    "        \n",
    "    fil1.close()\n",
    "    fil2.close()\n",
    "    final.close()\n",
    "    os.remove(file1)\n",
    "    os.remove(file2)\n",
    "    os.rename('index/temp.txt',file1)\n",
    "    \n",
    "merge_func('index/0.txt','index/1.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0b4|0b3c1t1'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"|\".join(\"world|0b4|0b3c1t1|\".split(\"|\")[1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "3\n",
      "5\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,9,2):\n",
    "            print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = ['18.txt','2.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp.sort(key = functools.cmp_to_key(comp) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['18.txt', '2.txt']"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
